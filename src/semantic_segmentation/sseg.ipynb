{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b99fbf6f-e9f5-4469-9385-099ddea81f95",
   "metadata": {},
   "source": [
    "# Semantic segmentation experiments\n",
    "\n",
    "Using vanilla PyTorch\n",
    "\n",
    "Faisal Qureshi      \n",
    "faisal.qureshi@ontariotechu.ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6cd02b-8df6-4ccb-a008-6af07b055c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '../../data/pinyon-jay-bird.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532deef2-6582-42ec-8338-9042178936a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ec3287-139e-4c96-a38e-6be4fb4935c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(filepath)\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698c33ad-bdb1-4003-91da-e0a984bb1421",
   "metadata": {},
   "source": [
    "We need to process this image before we can perform semantic segmentation on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0895358d-9ae0-4142-a5dd-317b1f673964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02008b6a-1cb0-45cf-ba6e-d824038f19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.CenterCrop((244, 244)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "img_transformed = transforms(img)\n",
    "print(img_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d410a2-f0c9-4c74-a445-b1004bea2347",
   "metadata": {},
   "source": [
    "Lets load a segmentation model and set it up for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d7cabe-42b7-4f76-9b1e-07c85a98cace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290a66df-910f-4a69-8913-32887fa4ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn = models.segmentation.fcn_resnet101(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974bfc7b-8978-496b-955d-d93eb866eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7dbd0b-7d83-44bc-b29f-543f57f9929f",
   "metadata": {},
   "source": [
    "We are ready to perform the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77f07dd-baf6-469a-9f19-c88b63c9a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = fcn(img_transformed.unsqueeze(0))\n",
    "print(output['out'].shape)\n",
    "print(output['aux'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca1b07-7bab-4a92-854a-1a498548a0ea",
   "metadata": {},
   "source": [
    "The output is a dict.  We can pull out the relevant tensor using `output['out']`.  Note that in this case the output tensor has 21 channels.  This is because this model was trained on 21 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d130281e-2352-4d3d-8964-d9d52e773992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc5879a-65ee-48a2-adce-3c2418f0658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_map = torch.argmax(output['out'].squeeze(), dim=0).detach().cpu().numpy()\n",
    "print(seg_map.shape)\n",
    "\n",
    "seg_map_aux = torch.argmax(output['aux'].squeeze(), dim=0).detach().cpu().numpy()\n",
    "print(seg_map_aux.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c926786-f922-4b3b-b385-0782a1d730f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_segmentation_map(seg_map, label_colors):\n",
    "    \"\"\"\n",
    "    seg_map is n-by-h-by-w output tensor as seen above.=\n",
    "    \n",
    "    classes is a n-by-3 colormap, where n is the \n",
    "    number of classes.\n",
    "    \"\"\"\n",
    "    r = np.zeros_like(seg_map)\n",
    "    g = np.zeros_like(seg_map)\n",
    "    b = np.zeros_like(seg_map)\n",
    "    for l in range(0, len(label_colors)):\n",
    "        idx = seg_map==l\n",
    "        r[idx] = label_colors[l, 0]\n",
    "        g[idx] = label_colors[l, 1]\n",
    "        b[idx] = label_colors[l, 2]\n",
    "    rgb = np.stack([r,g,b], axis=2)\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d820ccf-ba3d-4fb0-b517-95aeda2c9a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_colors = np.array([\n",
    "    # 0=background\n",
    "    (0, 0, 0),  \n",
    "    # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\n",
    "    (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),\n",
    "    # 6=bus, 7=car, 8=cat, 9=chair, 10=cow\n",
    "    (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0),\n",
    "    # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person\n",
    "    (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128),\n",
    "    # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n",
    "    (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)\n",
    "])\n",
    "\n",
    "print(label_colors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5695ce1c-24a5-4337-add7-a8f9eb465763",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(vis_segmentation_map(seg_map, label_colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43667bf7-5980-414a-8508-6669ccac18b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(vis_segmentation_map(seg_map_aux, label_colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179dd396-d626-426c-9c1e-6c28ab3e1946",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee87dea7-9dc3-4d28-b22c-4b6d36cc43db",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dict(fcn.named_parameters())\n",
    "print(p.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b968dd-b578-4bcb-b53b-9b972f3987e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchviz\n",
    "torchviz.make_dot(output['out'], params=dict(list(fcn.named_parameters()))).render(\"fcn_torchviz\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b0f88-aca3-45d7-9d14-cf5dc82666f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
