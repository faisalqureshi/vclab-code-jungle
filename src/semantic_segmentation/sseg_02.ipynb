{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d925adc-f4d1-4868-b5c3-526535298a11",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experiments with semantic segmentation \n",
    "\n",
    "Faisal Qureshi      \n",
    "faisal.qureshi@ontariotechu.ca\n",
    "\n",
    "Setting up a semantic segmentation pipeline in PyTorch for Oxford IIIT Pet Dataset.  \n",
    "\n",
    "## Readme\n",
    "\n",
    "- The goal is to learn PyTorch Lightening Package and Albumentations Transformation Package.\n",
    "- Instal segmentation_models_pytorch as `pip3 install git+https://github.com/qubvel/segmentation_models.pytorch@8bf52c7e862af006e76a23aae6aa17977d7f9a79`.  This code may not work with other versions of segmentation_models_pytorch module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762a523-fe26-49d5-981c-db59ec5427c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import numpy as np\n",
    "import albumentations as albu\n",
    "import segmentation_models_pytorch as smp\n",
    "import pytorch_lightning as pl\n",
    "import tqdm\n",
    "import pickle\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import copy\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6437dd1-d0a3-4e82-a676-7f9da14ef017",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "I downloaded the dataset from [https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b1c9a3-78d8-4f61-a62c-8b2e10058b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../../data/oxford-3t-pet-dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea20200-19f8-4c46-9ccf-11cc14fdaf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = os.listdir(os.path.join(DATA_FOLDER, 'images'))\n",
    "trimaps = os.listdir(os.path.join(DATA_FOLDER, 'annotations/trimaps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e647c-2224-446f-aae2-9c3e69e3f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307de977-c68d-43e2-a224-740a773b6c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = images[0]\n",
    "img = cv2.imread(os.path.join(DATA_FOLDER, 'images', i))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "mask = cv2.imread(os.path.join(DATA_FOLDER, 'annotations/trimaps', os.path.splitext(i)[0]+'.png'), 0)\n",
    "\n",
    "visualize(image=img, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46084b-0bde-4111-9c4e-e55e3e23028a",
   "metadata": {},
   "source": [
    "### Constructing the Dataset Object\n",
    "\n",
    "The dataset uses albumentations augmentations library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc62855-7aba-4449-969f-9856e76ac267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir,\n",
    "        masks_dir,\n",
    "        class_values = None,\n",
    "        augmentation = None,\n",
    "        preprocessing = None\n",
    "    ):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.class_values = class_values\n",
    "        self.num_classes = len(class_values)\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.idx = np.array([])\n",
    "        \n",
    "        try:\n",
    "            with open('oxford-3t-pets-valid-files.pkl', 'rb') as f:\n",
    "                self.files = pickle.load(f)['files']\n",
    "        except:\n",
    "            # Getting rid of unreadable image files\n",
    "            images = [os.path.splitext(i)[0] for i in os.listdir(images_dir) if os.path.splitext(i)[1] == '.jpg']\n",
    "            images_ = []\n",
    "            print('Checking image files...')\n",
    "            with tqdm.tqdm(total=len(images), position=0, leave=True) as pbar:\n",
    "                for i in tqdm.tqdm(range(len(images)), position=0, leave=True):\n",
    "                    filename = images[i]\n",
    "                    try:\n",
    "                        image = cv2.imread(os.path.join(self.images_dir, filename+'.jpg'))\n",
    "                        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                        images_.append(filename)\n",
    "                    except:\n",
    "                        pass\n",
    "                    pbar.update()\n",
    "            images = images_\n",
    "            # print(len(images))\n",
    "            \n",
    "            # Getting rid of unreadable mask files\n",
    "            masks = [os.path.splitext(i)[0] for i in os.listdir(masks_dir) if os.path.splitext(i)[1] == '.png']\n",
    "            masks_ = []\n",
    "            print('Checking mask files')\n",
    "            with tqdm.tqdm(total=len(masks), position=0, leave=True) as pbar:\n",
    "                for i in tqdm.tqdm(range(len(masks)), position=0, leave=True):\n",
    "                    filename = masks[i]\n",
    "                    try:\n",
    "                        mask = cv2.imread(os.path.join(self.masks_dir, filename+'.png'), 0)\n",
    "                        masks_.append(filename)\n",
    "                    except:\n",
    "                        pass\n",
    "                    pbar.update()\n",
    "            masks = masks_\n",
    "            # print(len(masks))\n",
    "\n",
    "            # Selecting image/mask pairs - this avoids the situation of having an image, but no mask, or having\n",
    "            # a mask, but no image.\n",
    "            self.files = list(set(images).intersection(set(masks)))\n",
    "            with open('oxford-3t-pets-valid-files.pkl', 'wb') as f:\n",
    "                pickle.dump({'files': self.files}, f)\n",
    "            print('Saved oxford-3t-pets-valid-files.pkl')\n",
    "            \n",
    "        # print(f'# image/Mask pairs {len(self.files)}')\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        i = index if len(self.idx) == 0 else self.idx[index]\n",
    "        \n",
    "        filename = self.files[i]\n",
    "        image = cv2.imread(os.path.join(self.images_dir, filename+'.jpg'))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(os.path.join(self.masks_dir, filename+'.png'), 0)\n",
    "        \n",
    "        masks = [(mask == v) for v in self.class_values]\n",
    "        mask = np.stack(masks, axis=-1).astype('float')\n",
    "        \n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files) if len(self.idx) == 0 else len(self.idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfdbc45-d4cc-4bcf-9493-be419957ed98",
   "metadata": {},
   "source": [
    "\n",
    "            'loss': loss,Checking if the dataset works without any augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a947b0f2-7ef0-4eec-8a8e-93c96bf90cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = os.path.join(DATA_FOLDER, 'images')\n",
    "MASK_FOLDER = os.path.join(DATA_FOLDER, 'annotations/trimaps')\n",
    "CLASS_VALUES = [1,2,3]  # 1: foreground, 2: background, and 3: not classified\n",
    "                        # These are mapped to 0, 1, 2 channels, respectively.\n",
    "\n",
    "dataset = Dataset(IMAGE_FOLDER, MASK_FOLDER, CLASS_VALUES)\n",
    "print(f'items in dataset = {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773cd9d5-c3b7-4427-ac33-1c0f77817a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, mask = dataset[34]\n",
    "visualize(image=image, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53beefea-a0b4-4239-9197-b19140fd57bc",
   "metadata": {},
   "source": [
    "#### Looping over all data items to catch any issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ffc64a-a541-4646-8b26-370345178a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(dataset)):\n",
    "#     try:\n",
    "#         image, mask = dataset[i]\n",
    "#     except:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f78618b-d60b-4f01-a9e7-15633f964ddd",
   "metadata": {},
   "source": [
    "### Defining Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b902b089-29d6-4dcb-90a0-b2ae1792cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [\n",
    "\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0),\n",
    "        albu.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n",
    "        albu.RandomCrop(height=320, width=320, always_apply=True),\n",
    "        albu.GaussNoise(p=0.2),\n",
    "        albu.IAAPerspective(p=0.5),\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.CLAHE(p=1),\n",
    "                albu.RandomBrightness(p=1),\n",
    "                albu.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.IAASharpen(p=1),\n",
    "                albu.Blur(blur_limit=3, p=1),\n",
    "                albu.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "        albu.OneOf(\n",
    "            [\n",
    "                albu.RandomContrast(p=1),\n",
    "                albu.HueSaturationValue(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "    ]\n",
    "    return albu.Compose(train_transform)\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n",
    "    test_transform = [\n",
    "        albu.LongestMaxSize(320),\n",
    "        albu.PadIfNeeded(min_height=320, min_width=320)\n",
    "    ]\n",
    "    return albu.Compose(test_transform)\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb54fd-78de-40d1-9428-3558a01f7467",
   "metadata": {},
   "source": [
    "#### Checking out augmented dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a8703d-3300-4f0f-b611-394001994d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = Dataset(IMAGE_FOLDER, MASK_FOLDER, CLASS_VALUES, get_training_augmentation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16cacb8-df20-4a05-bca4-b1cd442ee056",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    image, mask = augmented_dataset[1] # pick the same item three times\n",
    "    visualize(image=image, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3cbe10-9089-44f2-bbf2-01c64ebb9ae7",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now we construct a semantic segmentation model.  We will use `segmentation_model_pytorch` library to setup our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dbdcdc-ce62-402d-a0d1-d8d7f4c59598",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'se_resnext50_32x4d' \n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "ACTIVATION = 'softmax2d'\n",
    "NUM_CLASSES = 3\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605faf7c-3bd0-492f-91d1-2f700dd58160",
   "metadata": {},
   "source": [
    "Select the appropriate preprocessing function, which are used to normalize the images correctly for a particular model.  The following will be used within the dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac2819-2ea1-4c8f-b39f-4a9bb1368455",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "print(preprocessing_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcc53e7-0089-4884-8a38-5ac77f579105",
   "metadata": {},
   "source": [
    "### Constructing training, validation, and test datasets\n",
    "\n",
    "Since the dataset objects use different augmentations for training and validation/test, we will split the dataset without augmentations first and then we will set the augmentations accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f2e07a-70c8-4f0a-9b8d-702a9a2aea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(\n",
    "    IMAGE_FOLDER, \n",
    "    MASK_FOLDER, \n",
    "    CLASS_VALUES, \n",
    "    preprocessing=get_preprocessing(preprocessing_fn)\n",
    ")\n",
    "\n",
    "n_total = len(dataset)\n",
    "n_train = int(0.8*n_total)\n",
    "n_valid = int(0.2*n_total)\n",
    "\n",
    "idx = np.arange(n_total)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "valid_idx = idx[:n_valid]\n",
    "train_idx = idx[n_valid:n_train]\n",
    "test_idx = idx[n_train:]\n",
    "\n",
    "train_dataset = copy.deepcopy(dataset)\n",
    "train_dataset.idx = train_idx\n",
    "train_dataset.augmentation = get_training_augmentation()\n",
    "\n",
    "valid_dataset = copy.deepcopy(dataset)\n",
    "valid_dataset.idx = valid_idx\n",
    "valid_dataset.augmentation = get_validation_augmentation()\n",
    "\n",
    "test_dataset = copy.deepcopy(dataset)\n",
    "test_dataset.idx = test_idx\n",
    "test_dataset.augmentation = get_validation_augmentation()\n",
    "\n",
    "print(f'dataset = {len(dataset)}')\n",
    "print(f'train dataset = {len(train_dataset)}')\n",
    "print(f'valid dataset = {len(valid_dataset)}')\n",
    "print(f'test dataset = {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff149bb-d011-4895-b79b-684ee27cc08c",
   "metadata": {},
   "source": [
    "### Set up the training, validation and test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edbf44c-8f9b-48d2-8288-c2165cde0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=8)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db3604e-85fe-4dd9-9759-31fd82fbb754",
   "metadata": {},
   "source": [
    "### Model, Loss, error metrics\n",
    "\n",
    "PetModel is dervied from pl.LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc282d8-9181-4f42-9a2e-abcb371e23a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = smp.FPN(\n",
    "            encoder_name = ENCODER,\n",
    "            encoder_weights = ENCODER_WEIGHTS,\n",
    "            classes = NUM_CLASSES,\n",
    "            activation = ACTIVATION\n",
    "        )\n",
    "            'loss': loss,\n",
    "        \n",
    "        self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = self.model(x)\n",
    "        return mask\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def step(self, stage, batch, batch_idx):\n",
    "        x, y = batch # x is image and y is mask\n",
    "        yhat = self.forward(x)\n",
    "        loss = self.loss_fn(yhat, y)\n",
    "        \n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(yhat.long(), y.long(), mode='binary', threshold=0.5)\n",
    "        \n",
    "        return {\n",
    "            'loss': loss, # this is required by PyTorch Lightning framework.\n",
    "            'tp': tp, \n",
    "            'fp': fp,\n",
    "            'fn': fn,\n",
    "            'tn': tn            \n",
    "        }\n",
    "        \n",
    "    def step_end(self, stage, outputs):\n",
    "        pass\n",
    "        \n",
    "    def epoch_end(self, stage, outputs):      \n",
    "        # aggregate step metrics\n",
    "        loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        tp = torch.cat([x['tp'] for x in outputs])\n",
    "        fp = torch.cat([x['fp'] for x in outputs])\n",
    "        fn = torch.cat([x['fn'] for x in outputs])\n",
    "        tn = torch.cat([x['tn'] for x in outputs])\n",
    "        \n",
    "        # per image IoU means that we first calculate IoU score for each image \n",
    "        # and then compute mean over these scores\n",
    "        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n",
    "        \n",
    "        # dataset IoU means that we aggregate intersection and union over whole dataset\n",
    "        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n",
    "        # in this particular case will not be much, however for dataset \n",
    "        # with \"empty\" images (images without target class) a large gap could be observed. \n",
    "        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n",
    "        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n",
    "  \n",
    "        metrics = {\n",
    "            f'{stage}_loss': loss,\n",
    "            f'{stage}_per_image_iou': per_image_iou,\n",
    "            f'{stage}_dataset_iou': dataset_iou,\n",
    "        }\n",
    "        \n",
    "        self.log_dict(metrics)\n",
    "        \n",
    "        # self.logger.experiment.add_scalar(f'loss/{stage}', loss, self.current_epoch)\n",
    "    \n",
    "    # train\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step('train', batch, batch_idx)\n",
    "    \n",
    "    def training_step_end(self, outputs):\n",
    "        return self.step_end('train', outputs)\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        return self.epoch_end('train', outputs)\n",
    "    \n",
    "    # test\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step('test', batch, batch_idx)\n",
    "    \n",
    "    def test_step_end(self, outputs):\n",
    "        return self.step_end('test', outputs)\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        return epoch_end('test', outputs)\n",
    "    \n",
    "    # evaluate\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step('valid', batch, batch_idx)\n",
    "\n",
    "    def validation_step_end(self, outputs):\n",
    "        return self.step_end('valid', outputs)\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.epoch_end('valid', outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376ba49a-7a7d-4f43-bb64-0f4bf3638567",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a33a7-c0a7-41b1-ba6a-b6e861f496bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(monitor='loss', dirpath='./ckpt')\n",
    "logger = TensorBoardLogger('tb_logs', name='pet_model_1')\n",
    "\n",
    "model = PetModel()\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=5, logger=logger, callbacks=[checkpoint_callback])\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders = train_dataloader,\n",
    "    val_dataloaders = valid_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcfe192-58f0-43ae-bdcd-d5c4ca53bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f514996-360c-4393-a1f7-0cb6542a7658",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(gpus=1, max_epochs=10, logger=logger, callbacks=[checkpoint_callback])\n",
    "trainer.fit(model, train_dataloader, ckpt_path=checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f7c99-937a-4fd8-9270-3ce6ea05873d",
   "metadata": {},
   "source": [
    "## Validation and test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b35193-27fc-407b-ba2e-0b5e48049b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_metrics = trainer.validate(model, dataloaders=valid_dataloader, verbose=False)\n",
    "pprint(valid_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae56f3-eb19-43c2-83eb-7e7927eb3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = trainer.validate(model, dataloaders=test_dataloader, verbose=False)\n",
    "pprint(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c414859d-3679-4249-8c74-f069d94a2d24",
   "metadata": {},
   "source": [
    "## Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4906506a-6527-4a13-945e-9b330b63915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, gt_masks = next(iter(test_dataloader))\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    masks = model(images)\n",
    "    \n",
    "for image, gt_mask, mask in zip(images, gt_masks, masks):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(image.numpy().transpose(1,2,0))\n",
    "    plt.axis('off')\n",
    "    plt.title('image')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(gt_mask.numpy().transpose(1,2,0))\n",
    "    plt.axis('off')\n",
    "    plt.title('gt mask')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(mask.numpy().transpose(1,2,0))\n",
    "    plt.axis('off')\n",
    "    plt.title('mask')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebea6de-259d-4412-9819-97f1f8bc23ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
